{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Решение тестового задания на DS стажировку в Avito\n",
        "\n",
        "Задача: хотим угадывать, где пропущен пробел в данных.\n",
        "\n",
        "Краткий обзор методов:\n",
        "\n",
        "1) Языковые модели(RoBERTa, BERT, T5) -- использовать из коробки, заствляя BERT поедсказывать пробельный токен не получилось. Закономерный шаг -- затюнить. Ограничения с моей стороны -- долго обучать. Сбор данных вышел бы простой -- набрать тектов, повыбирать оттуда чанки предложений на 2-7 слов.\n",
        "\n",
        "2) Использование токенизаторов -- тут не удалось докуртить идею до чего-то разумного, строка может разбиваться на токены самыми странными способами\n",
        "\n",
        "3) ДП, частотные словари и эвристики показались довольно перспективным вариантом для, по крайней мере, бейзлайна.\n",
        "\n"
      ],
      "metadata": {
        "id": "9nnJjJWmH8CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import math"
      ],
      "metadata": {
        "id": "GOQZ1Jj2sBSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем частотный словарь (с трудом нашел в интернете по этой ссылке https://www.artint.ru/projects/frqlist/frqlist-en.php?utm_source=chatgpt.com)\n",
        "# Краткое описание: содержит порядка 32К слов с статистикой встречаемости. Собран по большому корпусу текстов.\n",
        "def load_freq_dict(path=\"./words.num\"):\n",
        "    freqs = {}\n",
        "    total = 0\n",
        "    with open(path, 'r', encoding='cp1251') as f:\n",
        "        for line in f:\n",
        "            num, freq, word = line.strip().split()\n",
        "            freq = int(float(freq))\n",
        "            freqs[word] = freq\n",
        "            total += freq\n",
        "    # нормализуем и логарифмируем, т.к. работаем с маленькими числами\n",
        "    probs = {w: math.log(f / total) for w, f in freqs.items()}\n",
        "    return probs\n",
        "\n",
        "# Алгоритм сегментации через динамическое программирование\n",
        "def segment(text, probs, max_word_len=20):\n",
        "    n = len(text)\n",
        "    dp = [-float(\"inf\")] * (n + 1)   # хранит максимальный логарифм вероятности разбиения подстроки text[:i]\n",
        "    back = [-1] * (n + 1)           # хранит индекс, откуда мы пришли в i(будем восстанавливать сегментацию)\n",
        "    dp[0] = 0\n",
        "\n",
        "    for i in range(1, n + 1): # индекс начала слова\n",
        "        for l in range(1, min(max_word_len, i) + 1): # длина слова\n",
        "            word = text[i - l:i]\n",
        "            if word in probs:\n",
        "                score = dp[i - l] + probs[word] # копим сумму логарифмов правдоподобия\n",
        "                if score > dp[i]:\n",
        "                    dp[i] = score\n",
        "                    back[i] = i - l # запомнили, где началось слово\n",
        "\n",
        "    # восстановаем разбиение\n",
        "    words = []\n",
        "    i = n\n",
        "    while i > 0 and back[i] != -1:\n",
        "        words.append(text[back[i]:i])\n",
        "        i = back[i]\n",
        "    words.reverse()\n",
        "    return words\n",
        "\n",
        "# проверим, что работает\n",
        "probs = load_freq_dict()\n",
        "\n",
        "text = \"елкиипалки\"\n",
        "print(\" \".join(segment(text, probs)))"
      ],
      "metadata": {
        "id": "gmn2CO6qc0xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод выше прямо использоваться не будем, мы обвесим его эвристиками.\n",
        "\n",
        "Мотивация эвристик:\n",
        "\n",
        "1) Есть слова на латинице, которые мешают обработки(плюс понятно, что это слова собственные и их надо выделять)\n",
        "2) -, будем считать, выступает как в конструкции \"объект - это объект\"\n",
        "3) после запятой пробел, до неё -- нет\n",
        "4) цифры тоже выделяем отдельно"
      ],
      "metadata": {
        "id": "2wRlPHDLPFz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def predict_space_positions(text, probs):\n",
        "    positions = []\n",
        "    text = text.lower() # нужно привести в нижний регистр, чтобы всё работало\n",
        "\n",
        "    # 1) Эвристики\n",
        "    for i, ch in enumerate(text):\n",
        "        # 1.1) цифры\n",
        "        if ch.isdigit():\n",
        "            if i > 0 and text[i-1].isalpha():\n",
        "                positions.append(i)\n",
        "            if i+1 < len(text) and text[i+1].isalpha():\n",
        "                positions.append(i+1)\n",
        "\n",
        "        # 1.2) латиница\n",
        "        if ch.isalpha() and ('a' <= ch.lower() <= 'z'):\n",
        "            if i > 0 and text[i-1].isalpha() and ('а' <= text[i-1].lower() <= 'я'):\n",
        "                positions.append(i)\n",
        "            if i+1 < len(text) and ('а' <= text[i+1].lower() <= 'я'):\n",
        "                positions.append(i+1)\n",
        "\n",
        "        # 1.3) запятая\n",
        "        if ch == ',' and i+1 < len(text) and text[i+1] != ' ':\n",
        "            positions.append(i+1)\n",
        "\n",
        "        # 1.4) дефис\n",
        "        if ch == '-':\n",
        "            if i > 0 and text[i-1] != ' ':\n",
        "                positions.append(i)\n",
        "            if i+1 < len(text) and text[i+1] != ' ':\n",
        "                positions.append(i+1)\n",
        "\n",
        "    # Алгоритм с ДП сегментацией\n",
        "    clean = re.sub(r'[^а-яА-Я]', '', text.lower()) # Удаляем все, о чем говорили выше, чтобы не мешало обработке\n",
        "    words = segment(clean, probs)\n",
        "    idx = 0\n",
        "    for w in words[:-1]:\n",
        "        idx = text.find(w, idx) + len(w)\n",
        "        if idx < len(text) and text[idx] != ',': # перед запятой не может быть пробела\n",
        "            positions.append(idx)\n",
        "\n",
        "    # на всякий случай, убираем будли\n",
        "    return sorted(set(positions))\n",
        "\n",
        "\n",
        "test_predict = 'куплюxiaomi5500'\n",
        "predict_space_positions(test_predict, probs)"
      ],
      "metadata": {
        "id": "r0i-rRhvO0wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./dataset_1937770_3.txt', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "processed_lines = []\n",
        "for line in lines:\n",
        "    parts = line.strip().split(',', 1)\n",
        "    if len(parts) == 2:\n",
        "        processed_lines.append(parts)\n",
        "\n",
        "task_data = pd.DataFrame(processed_lines[1:], columns=processed_lines[0])\n",
        "task_data.head()"
      ],
      "metadata": {
        "id": "5pF8CiXDrP4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_data['predicted_positions'] = task_data['text_no_spaces'].apply(lambda x: str(predict_space_positions(x, probs)))\n",
        "task_data.head(10)"
      ],
      "metadata": {
        "id": "hzEXTX-9qQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_submit = task_data.drop('text_no_spaces', axis=1)\n",
        "to_submit.to_csv('submit_dp_heur_1.txt')"
      ],
      "metadata": {
        "id": "95rnI5rysoPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Итог\n",
        "\n",
        "Получили бейзлайн, показывающий на паблике 87,5% при том с очень дешевым инференсом, работающим за линейное время.\n",
        "\n",
        "Интернет говорит, что это похоже на алгоритм Витерби."
      ],
      "metadata": {
        "id": "TUbLmF1BRjvv"
      }
    }
  ]
}